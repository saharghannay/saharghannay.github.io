<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Semantic Representations | Sahar Ghannay</title>
    <link>https://saharghannay.github.io/courses/cours1/</link>
      <atom:link href="https://saharghannay.github.io/courses/cours1/index.xml" rel="self" type="application/rss+xml" />
    <description>Semantic Representations</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Wed, 05 Sep 2018 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://saharghannay.github.io/img/icon-192.png</url>
      <title>Semantic Representations</title>
      <link>https://saharghannay.github.io/courses/cours1/</link>
    </image>
    
    <item>
      <title>Word embeddings training</title>
      <link>https://saharghannay.github.io/courses/cours1/example1/</link>
      <pubDate>Sat, 05 May 2018 00:00:00 +0100</pubDate>
      <guid>https://saharghannay.github.io/courses/cours1/example1/</guid>
      <description>&lt;p&gt;[English version below]&lt;/p&gt;
&lt;h1 id=&#34;objectif&#34;&gt;Objectif&lt;/h1&gt;
&lt;p&gt;L&amp;rsquo;objectif du TP est de construire et de comparer différents type de plongements lexicaux (embeddings de mots) en utilisant les bibliothèques &lt;code&gt;gensim&lt;/code&gt; et &lt;code&gt;fasttext&lt;/code&gt;.&lt;br&gt;
Ces embeddings seront entrainés sur deux corpus différents : corpus en domaine médical (QUAERO_FrenchMed) de petite taille et un corpus non médical (QUAERO_FrenchPress) de grande taille.
Ils seront évalués sur la tâche de détection d’entités nommées (NER : Named Entity recognition)  pendant le TP2 (l’après midi).&lt;/p&gt;
&lt;p&gt;Vous êtes invités à utiliser les approches &lt;strong&gt;word2vec&lt;/strong&gt; (Cbow, skipgram) et &lt;strong&gt;fasttext&lt;/strong&gt; (Cbow).&lt;/p&gt;
&lt;h1 id=&#34;ressources&#34;&gt;Ressources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Word2vec : &lt;a href=&#34;https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec&#34;&gt;https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fasttext : &lt;a href=&#34;https://fasttext.cc/docs/en/support.html&#34;&gt;https://fasttext.cc/docs/en/support.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Corpus: &lt;a href=&#34;https://perso.limsi.fr/neveol/TP_ISD2020.zip&#34;&gt;https://perso.limsi.fr/neveol/TP_ISD2020.zip&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Les fichiers &lt;code&gt;QUAERO_FrenchMed_traindev.ospl&lt;/code&gt; et &lt;code&gt;QUAERO_FrenchPress_traindev.ospl&lt;/code&gt; seront utilisés pour l&amp;rsquo;apprentissage des embeddings. Ils contiennent des corpus au format «une phrase par ligne», avec une segmentation des tokens qui sont séparés par des espaces.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;outils-nécessaires&#34;&gt;Outils nécessaires&lt;/h1&gt;
&lt;p&gt;Il est recommandé d&amp;rsquo;utiliser google colab pour travailler le TP.
Avant de commencer il faut installer les outils nécessaires.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# Installer les outils nécessaires pour la création des embeddings
  pip install gensim 
  pip install fasttext 
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;apprentissage-des-embeddings-de-mot&#34;&gt;Apprentissage des embeddings de mot&lt;/h1&gt;
&lt;p&gt;La première étape de votre travail va être de créer des scripts python et bash permettant d&amp;rsquo;apprendre les différentes approches d’embeddings &lt;strong&gt;word2vec&lt;/strong&gt; (Cbow, skipgram) et &lt;strong&gt;fasttext&lt;/strong&gt; (Cbow) sur les deux corpus médical et non médical &lt;strong&gt;QUAERO_FrenchMed&lt;/strong&gt; et &lt;strong&gt;QUAERO_FrenchPress&lt;/strong&gt; (au total 6 embeddings).&lt;/p&gt;
&lt;p&gt;Suivez les étapes nécessaires dans la documentation pour créer et sauvegarder les modèles et les embeddings de mots.&lt;/p&gt;
&lt;p&gt;Vous pouvez utiliser ces hyper-paramètres pour l&amp;rsquo;apprentissage des embeddings : &lt;code&gt;dim=100&lt;/code&gt;, &lt;code&gt;min_count=1&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;similarité-sémantique&#34;&gt;Similarité sémantique&lt;/h1&gt;
&lt;p&gt;La deuxième étape consiste à trouver les mots les plus proches d&amp;rsquo;un mot donné en s&amp;rsquo;appuyant sur le calcul de similarité cosinus.
Plusieurs évaluations à faire :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Comparer des  d&amp;rsquo;embeddings entrainés sur le même corpus :
&lt;ul&gt;
&lt;li&gt;tester l&amp;rsquo;impact des approches (skipgram, cbow, fasttext) sur le résultats&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Comparer des embeddings (même approche) entrainés sur de corpus différents (médical et non médical) :
&lt;ul&gt;
&lt;li&gt;tester l&amp;rsquo;impact de données (type et quantité) sur les résultats&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Voici la liste de mots candidats :  &lt;code&gt;patient, traitement, maladie, solution, jaune&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pour l&amp;rsquo;évaluation vous pouvez utiliser soit :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;la méthode &lt;code&gt;spatial&lt;/code&gt; de la bibliothèque python scipy, dans ce cas, vous devez charger les embeddings construits, et cherchez pour un mot donné la liste des 10 mots les plus proches&lt;/li&gt;
&lt;li&gt;la méthode &lt;code&gt;most_similar&lt;/code&gt; du gensim, dans ce cas, vous devez charger les modèles sauvegardés
&lt;ul&gt;
&lt;li&gt;vous pouvez utiliser gensim pour charger les modèles word2vec et fasttext&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;objective&#34;&gt;Objective&lt;/h1&gt;
&lt;p&gt;The objective of this lab session is to build and compare different word embedding approaches using the &lt;code&gt;gensim&lt;/code&gt; and &lt;code&gt;fasttext&lt;/code&gt; libraries.
These embeddings will be trained on two different corpora: a small medical corpus (QUAERO_FrenchMed) and a large non-medical corpus (QUAERO_FrenchPress). They will be evaluated on the NER: Named Entity recognition task during the second practical session.&lt;/p&gt;
&lt;p&gt;You are invited to use the &lt;strong&gt;word2vec&lt;/strong&gt; (Cbow, skipgram) and &lt;strong&gt;fasttext&lt;/strong&gt; (Cbow) approaches.&lt;/p&gt;
&lt;h1 id=&#34;ressources-1&#34;&gt;Ressources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Word2vec : &lt;a href=&#34;https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec&#34;&gt;https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fasttext : &lt;a href=&#34;https://fasttext.cc/docs/en/support.html&#34;&gt;https://fasttext.cc/docs/en/support.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Corpus: &lt;a href=&#34;https://perso.limsi.fr/neveol/TP_ISD2020.zip&#34;&gt;https://perso.limsi.fr/neveol/TP_ISD2020.zip&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;The files &lt;code&gt;QUAERO_FrenchMed_traindev.ospl&lt;/code&gt; et &lt;code&gt;QUAERO_FrenchPress_traindev.ospl&lt;/code&gt; will be used for learning embeddings. They contain corpora in “one sentence per line” format, with a segmentation of the tokens which are separated by spaces.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;requirements&#34;&gt;Requirements&lt;/h1&gt;
&lt;p&gt;It is recommended to use Google Colab to work on the TP. Before starting, you must install the necessary tools.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# Install the required tools to train word embeddings
  pip install gensim 
  pip install fasttext 
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;word-embeddings-training&#34;&gt;Word embeddings training&lt;/h1&gt;
&lt;p&gt;The first step of your work is to create python and bash scripts allowing you to train the different embeddings approaches: &lt;strong&gt;word2vec&lt;/strong&gt; (Cbow, skipgram) and &lt;strong&gt;fasttext&lt;/strong&gt; (Cbow), on the two medical and non-medical corpora, resulting to 6 embeddings models.&lt;/p&gt;
&lt;p&gt;Follow the steps in the documentation to create and save the models and the word embeddings.&lt;/p&gt;
&lt;p&gt;You can use these hyper-parameters to train the embeddings: &lt;code&gt;dim = 100&lt;/code&gt;,&lt;code&gt; min_count = 1&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;semantic-similarity&#34;&gt;Semantic similarity&lt;/h1&gt;
&lt;p&gt;The second step is to find the closest words to a given word based on the cosine similarity calculation.
Several evaluations to do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compare embeddings trained on the same corpus:
&lt;ul&gt;
&lt;li&gt;to test the impact of the embeddings approaches (skipgram, cbow, fasttext) on the results&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compare embeddings (same approach) trained on different corpora (medical and non-medical):
&lt;ul&gt;
&lt;li&gt;to test the impact of data (type and quantity) on the results&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is the candidate word list: &lt;code&gt;patient, treatment, disease, solution, yellow&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For the evaluation you can use either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spatial&lt;/code&gt; method of the python scipy library, in this case, you must load the embeddings vectors, and search for a given word for the list of the 10 closest words&lt;/li&gt;
&lt;li&gt;gensim&amp;rsquo;s &lt;code&gt;most_similar&lt;/code&gt; method, in this case you have to load the saved models
&lt;ul&gt;
&lt;li&gt;you can use gensim to load word2vec and fasttext models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Named entity recognition</title>
      <link>https://saharghannay.github.io/courses/cours1/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://saharghannay.github.io/courses/cours1/example2/</guid>
      <description>&lt;p&gt;[English version below]&lt;/p&gt;
&lt;h1 id=&#34;objectif&#34;&gt;Objectif&lt;/h1&gt;
&lt;p&gt;L&amp;rsquo;objectif du TP est d&amp;rsquo;évaluer la performance des embeddings appris dans le TP1 ainsi qu&amp;rsquo;un modèle Transformer (BERT) sur la tâche d&amp;rsquo;entités nommées (NER) dans le domaine médicale.
Un modèle NER par type d&amp;rsquo;embeddings sera appris et évalué.&lt;/p&gt;
&lt;h1 id=&#34;ressources&#34;&gt;Ressources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Les &lt;a href=&#34;https://saharghannay.github.io/files/scripts.zip&#34; target=&#34;_blank&#34;&gt; scripts &lt;/a&gt; à adapter pour la tâche de classification de séquence, un pour exécuter les modèles LSTM et CNN, un autre pour exécuter les modèles Transformer (BERT)&lt;/li&gt;
&lt;li&gt;Corpus: &lt;a href=&#34;https://perso.limsi.fr/neveol/TP_ISD2020.zip&#34;&gt;https://perso.limsi.fr/neveol/TP_ISD2020.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;outils-nécessaires&#34;&gt;Outils nécessaires&lt;/h1&gt;
&lt;p&gt;Veuillez installer les dépendances nécessaires pour executer les scripts&lt;/p&gt;
&lt;h1 id=&#34;présentation-rapide-des-corpus&#34;&gt;Présentation rapide des corpus :&lt;/h1&gt;
&lt;p&gt;Nous utiliserons un corpus annoté en entités issu du domaine médical (QUAERO_FrenchMed) de petit taille.&lt;/p&gt;
&lt;p&gt;Ce corpus vous est fourni dans un format similaire au format conll : il contient cinq colonnes séparées par des espaces.&lt;/p&gt;
&lt;p&gt;Chaque mot correspond à une ligne et les phrases sont séparées par une ligne vide.
Les colonnes correspondent dans l&amp;rsquo;ordre à l&amp;rsquo;index du mot dans la phrase, le mot, deux autres colonnes qui ne seront pas utilisées et la dernière colonne représente l&amp;rsquo;étiquette d&amp;rsquo;entité nommée.&lt;/p&gt;
&lt;p&gt;Les étiquettes d’entité nommée sont au format I-TYPE qui indique que le mot fait partie d’une entité de type TYPE.
Si deux entités de même type se suivent, le premier mot de la seconde aura pour étiquette B-TYPE pour indiquer qu’il s&amp;rsquo;agit d’une nouvelle entité.
L’étiquette O indique que le mot ne fait pas partie d’une entité.&lt;/p&gt;
&lt;h1 id=&#34;création-de-modèles&#34;&gt;Création de modèles&lt;/h1&gt;
&lt;p&gt;Afin de réaliser le travail il faut adapter les scripts pour la tâche de NER (étiquetage de séquence): principalement le chargement des données, la fonction objective (si besoin)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;En plus de cela, dans le script cnn_classification.py, les embeddings sont initialisés aléatoirement, il faut faire les modifications nécessaires pour les initialisés avec les embeddings appris dans le TP1.&lt;/li&gt;
&lt;li&gt;Dans le script transformer_classification il faut changer &amp;lsquo;AutoModelForSequenceClassification&amp;rsquo;  par &amp;lsquo;AutoModelForTokenClassification&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Les résultats seront évaluer en termes de rappel, précision et F1&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Quel modèle obtient les meilleures performances ?&lt;/li&gt;
&lt;li&gt;Voyez vous des différences entre les embeddings appris sur deux corpus différents de TP1 ?&lt;/li&gt;
&lt;li&gt;Comparer ces résultats avec un modèle Transformer.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;objective&#34;&gt;Objective&lt;/h1&gt;
&lt;p&gt;The objective of the lab session is to evaluate the performance of the embeddings learned in TP1 as well as a Transformer model (BERT) on the named entity task (NER) in the medical domain.
A NER model for each embeddings type will be learned and evaluated.&lt;/p&gt;
&lt;h1 id=&#34;ressources-1&#34;&gt;Ressources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://saharghannay.github.io/files/scripts.zip&#34; target=&#34;_blank&#34;&gt; scripts &lt;/a&gt; to be adapted for the sequence classification task, one to run the LSTM and CNN models, another to run the Transformer (BERT) models&lt;/li&gt;
&lt;li&gt;Corpus: &lt;a href=&#34;https://perso.limsi.fr/neveol/TP_ISD2020.zip&#34;&gt;https://perso.limsi.fr/neveol/TP_ISD2020.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;requirements&#34;&gt;Requirements&lt;/h1&gt;
&lt;p&gt;Please install the dependencies to run the scripts&lt;/p&gt;
&lt;h1 id=&#34;quick-overview-of-the-datasets&#34;&gt;Quick overview of the datasets :&lt;/h1&gt;
&lt;p&gt;We will use a small medical corpus (QUAERO_FrenchMed) with named entity annotations.&lt;/p&gt;
&lt;p&gt;The dataset is provided in conll-like format and contain five columns separated by white spaces. Each word (or token) corresponds to a line and sentences are separated by an empty line.&lt;/p&gt;
&lt;p&gt;The columns correspond to: the token index within the sentence, the token, two columns that will not be used in this lab and finally, the last column contains the named entity tag.&lt;/p&gt;
&lt;p&gt;Named entity tags are in the I-TYPE format, which indicates that the word is part of a TYPE entity. If two entities follow each other, the first word of the second entity will get a B-TYPE tag to indicate that it is a new entity. The O tag indicates that the word is not part of an entity.&lt;/p&gt;
&lt;h1 id=&#34;ner-models-training&#34;&gt;NER models training&lt;/h1&gt;
&lt;p&gt;In order to do the work, it is necessary to adapt the scripts for the NER task (token classification): mainly the data loading, the objective function (if needed)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In addition to that, in the cnn_classification.py script, the embeddings are initialized randomly, it is necessary to make the modifications to initialize them with the embeddings learned in TP1.&lt;/li&gt;
&lt;li&gt;In the transformer_classification script, it is necessary to change &amp;lsquo;AutoModelForSequenceClassification&amp;rsquo; to &amp;lsquo;AutoModelForTokenClassification&amp;rsquo;&lt;/li&gt;
&lt;li&gt;The results will be evaluated in terms of recall, precision and F1&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;questions-1&#34;&gt;Questions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Which model provides the best performance on each dataset?&lt;/li&gt;
&lt;li&gt;Do you see any differences between the embeddings learned on two different corpora of TP1?&lt;/li&gt;
&lt;li&gt;Compare these results with a Transformer model.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
