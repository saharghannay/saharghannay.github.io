<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Sahar Ghannay</title>
    <link>https://saharghannay.github.io/</link>
      <atom:link href="https://saharghannay.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Sahar Ghannay</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Mon, 03 Jul 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://saharghannay.github.io/img/icon-192.png</url>
      <title>Sahar Ghannay</title>
      <link>https://saharghannay.github.io/</link>
    </image>
    
    <item>
      <title>Word embeddings training</title>
      <link>https://saharghannay.github.io/courses/cours1/example1/</link>
      <pubDate>Sat, 05 May 2018 00:00:00 +0100</pubDate>
      <guid>https://saharghannay.github.io/courses/cours1/example1/</guid>
      <description>&lt;p&gt;[English version below]&lt;/p&gt;
&lt;h1 id=&#34;objectif&#34;&gt;Objectif&lt;/h1&gt;
&lt;p&gt;L&amp;rsquo;objectif du TP est de construire et de comparer différents type de plongements lexicaux (embeddings de mots) en utilisant les bibliothèques &lt;code&gt;gensim&lt;/code&gt; et &lt;code&gt;fasttext&lt;/code&gt;.&lt;br&gt;
Ces embeddings seront entrainés sur deux corpus différents : corpus en domaine médical (QUAERO_FrenchMed) de petite taille et un corpus non médical (QUAERO_FrenchPress) de grande taille.
Ils seront évalués sur la tâche de détection d’entités nommées (NER : Named Entity recognition)  pendant le TP2 (l’après midi).&lt;/p&gt;
&lt;p&gt;Vous êtes invités à utiliser les approches &lt;strong&gt;word2vec&lt;/strong&gt; (Cbow, skipgram) et &lt;strong&gt;fasttext&lt;/strong&gt; (Cbow).&lt;/p&gt;
&lt;h1 id=&#34;ressources&#34;&gt;Ressources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Word2vec : &lt;a href=&#34;https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec&#34;&gt;https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fasttext : &lt;a href=&#34;https://fasttext.cc/docs/en/support.html&#34;&gt;https://fasttext.cc/docs/en/support.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Corpus: &lt;a href=&#34;https://perso.limsi.fr/neveol/TP_ISD2020.zip&#34;&gt;https://perso.limsi.fr/neveol/TP_ISD2020.zip&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Les fichiers &lt;code&gt;QUAERO_FrenchMed_traindev.ospl&lt;/code&gt; et &lt;code&gt;QUAERO_FrenchPress_traindev.ospl&lt;/code&gt; seront utilisés pour l&amp;rsquo;apprentissage des embeddings. Ils contiennent des corpus au format «une phrase par ligne», avec une segmentation des tokens qui sont séparés par des espaces.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;outils-nécessaires&#34;&gt;Outils nécessaires&lt;/h1&gt;
&lt;p&gt;Il est recommandé d&amp;rsquo;utiliser google colab pour travailler le TP.
Avant de commencer il faut installer les outils nécessaires.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# Installer les outils nécessaires pour la création des embeddings
  pip install gensim 
  pip install fasttext 
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;apprentissage-des-embeddings-de-mot&#34;&gt;Apprentissage des embeddings de mot&lt;/h1&gt;
&lt;p&gt;La première étape de votre travail va être de créer des scripts python et bash permettant d&amp;rsquo;apprendre les différentes approches d’embeddings &lt;strong&gt;word2vec&lt;/strong&gt; (Cbow, skipgram) et &lt;strong&gt;fasttext&lt;/strong&gt; (Cbow) sur les deux corpus médical et non médical &lt;strong&gt;QUAERO_FrenchMed&lt;/strong&gt; et &lt;strong&gt;QUAERO_FrenchPress&lt;/strong&gt; (au total 6 embeddings).&lt;/p&gt;
&lt;p&gt;Suivez les étapes nécessaires dans la documentation pour créer et sauvegarder les modèles et les embeddings de mots.&lt;/p&gt;
&lt;p&gt;Vous pouvez utiliser ces hyper-paramètres pour l&amp;rsquo;apprentissage des embeddings : &lt;code&gt;dim=100&lt;/code&gt;, &lt;code&gt;min_count=1&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;similarité-sémantique&#34;&gt;Similarité sémantique&lt;/h1&gt;
&lt;p&gt;La deuxième étape consiste à trouver les mots les plus proches d&amp;rsquo;un mot donné en s&amp;rsquo;appuyant sur le calcul de similarité cosinus.
Plusieurs évaluations à faire :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Comparer des  d&amp;rsquo;embeddings entrainés sur le même corpus :
&lt;ul&gt;
&lt;li&gt;tester l&amp;rsquo;impact des approches (skipgram, cbow, fasttext) sur le résultats&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Comparer des embeddings (même approche) entrainés sur de corpus différents (médical et non médical) :
&lt;ul&gt;
&lt;li&gt;tester l&amp;rsquo;impact de données (type et quantité) sur les résultats&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Voici la liste de mots candidats :  &lt;code&gt;patient, traitement, maladie, solution, jaune&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Pour l&amp;rsquo;évaluation vous pouvez utiliser soit :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;la méthode &lt;code&gt;spatial&lt;/code&gt; de la bibliothèque python scipy, dans ce cas, vous devez charger les embeddings construits, et cherchez pour un mot donné la liste des 10 mots les plus proches&lt;/li&gt;
&lt;li&gt;la méthode &lt;code&gt;most_similar&lt;/code&gt; du gensim, dans ce cas, vous devez charger les modèles sauvegardés
&lt;ul&gt;
&lt;li&gt;vous pouvez utiliser gensim pour charger les modèles word2vec et fasttext&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;objective&#34;&gt;Objective&lt;/h1&gt;
&lt;p&gt;The objective of this lab session is to build and compare different word embedding approaches using the &lt;code&gt;gensim&lt;/code&gt; and &lt;code&gt;fasttext&lt;/code&gt; libraries.
These embeddings will be trained on two different corpora: a small medical corpus (QUAERO_FrenchMed) and a large non-medical corpus (QUAERO_FrenchPress). They will be evaluated on the NER: Named Entity recognition task during the second practical session.&lt;/p&gt;
&lt;p&gt;You are invited to use the &lt;strong&gt;word2vec&lt;/strong&gt; (Cbow, skipgram) and &lt;strong&gt;fasttext&lt;/strong&gt; (Cbow) approaches.&lt;/p&gt;
&lt;h1 id=&#34;ressources-1&#34;&gt;Ressources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Word2vec : &lt;a href=&#34;https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec&#34;&gt;https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Fasttext : &lt;a href=&#34;https://fasttext.cc/docs/en/support.html&#34;&gt;https://fasttext.cc/docs/en/support.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Corpus: &lt;a href=&#34;https://perso.limsi.fr/neveol/TP_ISD2020.zip&#34;&gt;https://perso.limsi.fr/neveol/TP_ISD2020.zip&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;The files &lt;code&gt;QUAERO_FrenchMed_traindev.ospl&lt;/code&gt; et &lt;code&gt;QUAERO_FrenchPress_traindev.ospl&lt;/code&gt; will be used for learning embeddings. They contain corpora in “one sentence per line” format, with a segmentation of the tokens which are separated by spaces.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;requirements&#34;&gt;Requirements&lt;/h1&gt;
&lt;p&gt;It is recommended to use Google Colab to work on the TP. Before starting, you must install the necessary tools.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
# Install the required tools to train word embeddings
  pip install gensim 
  pip install fasttext 
&lt;/code&gt;&lt;/pre&gt;
&lt;h1 id=&#34;word-embeddings-training&#34;&gt;Word embeddings training&lt;/h1&gt;
&lt;p&gt;The first step of your work is to create python and bash scripts allowing you to train the different embeddings approaches: &lt;strong&gt;word2vec&lt;/strong&gt; (Cbow, skipgram) and &lt;strong&gt;fasttext&lt;/strong&gt; (Cbow), on the two medical and non-medical corpora, resulting to 6 embeddings models.&lt;/p&gt;
&lt;p&gt;Follow the steps in the documentation to create and save the models and the word embeddings.&lt;/p&gt;
&lt;p&gt;You can use these hyper-parameters to train the embeddings: &lt;code&gt;dim = 100&lt;/code&gt;,&lt;code&gt; min_count = 1&lt;/code&gt;.&lt;/p&gt;
&lt;h1 id=&#34;semantic-similarity&#34;&gt;Semantic similarity&lt;/h1&gt;
&lt;p&gt;The second step is to find the closest words to a given word based on the cosine similarity calculation.
Several evaluations to do:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Compare embeddings trained on the same corpus:
&lt;ul&gt;
&lt;li&gt;to test the impact of the embeddings approaches (skipgram, cbow, fasttext) on the results&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Compare embeddings (same approach) trained on different corpora (medical and non-medical):
&lt;ul&gt;
&lt;li&gt;to test the impact of data (type and quantity) on the results&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here is the candidate word list: &lt;code&gt;patient, treatment, disease, solution, yellow&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;For the evaluation you can use either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;spatial&lt;/code&gt; method of the python scipy library, in this case, you must load the embeddings vectors, and search for a given word for the list of the 10 closest words&lt;/li&gt;
&lt;li&gt;gensim&amp;rsquo;s &lt;code&gt;most_similar&lt;/code&gt; method, in this case you have to load the saved models
&lt;ul&gt;
&lt;li&gt;you can use gensim to load word2vec and fasttext models&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Named entity recognition</title>
      <link>https://saharghannay.github.io/courses/cours1/example2/</link>
      <pubDate>Sun, 05 May 2019 00:00:00 +0100</pubDate>
      <guid>https://saharghannay.github.io/courses/cours1/example2/</guid>
      <description>&lt;p&gt;[English version below]&lt;/p&gt;
&lt;h1 id=&#34;objectif&#34;&gt;Objectif&lt;/h1&gt;
&lt;p&gt;L&amp;rsquo;objectif du TP est d&amp;rsquo;évaluer la performance des embeddings appris dans le TP1 ainsi qu&amp;rsquo;un modèle Transformer (BERT) sur la tâche d&amp;rsquo;entités nommées (NER) dans le domaine médicale.
Un modèle NER par type d&amp;rsquo;embeddings sera appris et évalué.&lt;/p&gt;
&lt;h1 id=&#34;ressources&#34;&gt;Ressources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Les &lt;a href=&#34;https://saharghannay.github.io/files/scripts.zip&#34; target=&#34;_blank&#34;&gt; scripts &lt;/a&gt; à adapter pour la tâche de classification de séquence, un pour exécuter les modèles LSTM et CNN, un autre pour exécuter les modèles Transformer (BERT)&lt;/li&gt;
&lt;li&gt;Corpus: &lt;a href=&#34;https://perso.limsi.fr/neveol/TP_ISD2020.zip&#34;&gt;https://perso.limsi.fr/neveol/TP_ISD2020.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;outils-nécessaires&#34;&gt;Outils nécessaires&lt;/h1&gt;
&lt;p&gt;Veuillez installer les dépendances nécessaires pour executer les scripts&lt;/p&gt;
&lt;h1 id=&#34;présentation-rapide-des-corpus&#34;&gt;Présentation rapide des corpus :&lt;/h1&gt;
&lt;p&gt;Nous utiliserons un corpus annoté en entités issu du domaine médical (QUAERO_FrenchMed) de petit taille.&lt;/p&gt;
&lt;p&gt;Ce corpus vous est fourni dans un format similaire au format conll : il contient cinq colonnes séparées par des espaces.&lt;/p&gt;
&lt;p&gt;Chaque mot correspond à une ligne et les phrases sont séparées par une ligne vide.
Les colonnes correspondent dans l&amp;rsquo;ordre à l&amp;rsquo;index du mot dans la phrase, le mot, deux autres colonnes qui ne seront pas utilisées et la dernière colonne représente l&amp;rsquo;étiquette d&amp;rsquo;entité nommée.&lt;/p&gt;
&lt;p&gt;Les étiquettes d’entité nommée sont au format I-TYPE qui indique que le mot fait partie d’une entité de type TYPE.
Si deux entités de même type se suivent, le premier mot de la seconde aura pour étiquette B-TYPE pour indiquer qu’il s&amp;rsquo;agit d’une nouvelle entité.
L’étiquette O indique que le mot ne fait pas partie d’une entité.&lt;/p&gt;
&lt;h1 id=&#34;création-de-modèles&#34;&gt;Création de modèles&lt;/h1&gt;
&lt;p&gt;Afin de réaliser le travail il faut adapter les scripts pour la tâche de NER (étiquetage de séquence): principalement le chargement des données, la fonction objective (si besoin)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;En plus de cela, dans le script cnn_classification.py, les embeddings sont initialisés aléatoirement, il faut faire les modifications nécessaires pour les initialisés avec les embeddings appris dans le TP1.&lt;/li&gt;
&lt;li&gt;Dans le script transformer_classification il faut changer &amp;lsquo;AutoModelForSequenceClassification&amp;rsquo;  par &amp;lsquo;AutoModelForTokenClassification&amp;rsquo;&lt;/li&gt;
&lt;li&gt;Les résultats seront évaluer en termes de rappel, précision et F1&lt;/li&gt;
&lt;li&gt;Il est recommandé d’utiliser seqeval (&lt;a href=&#34;https://pypi.org/project/seqeval/&#34;&gt;https://pypi.org/project/seqeval/&lt;/a&gt;) pour l’évaluation.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Quel modèle obtient les meilleures performances ?&lt;/li&gt;
&lt;li&gt;Voyez vous des différences entre les embeddings appris sur deux corpus différents de TP1 ?&lt;/li&gt;
&lt;li&gt;Comparer ces résultats avec un modèle Transformer.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;recommandations&#34;&gt;Recommandations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Les rapports et scripts doivent être produits et envoyés avant la séance du 6 février.&lt;/li&gt;
&lt;li&gt;Ils résumeront le travail des deux séances de laboratoire.&lt;/li&gt;
&lt;li&gt;Pour la deuxième séance de laboratoire, le rapport résumera les résultats de la NER des différents modèles ainsi que les réponses aux questions.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;objective&#34;&gt;Objective&lt;/h1&gt;
&lt;p&gt;The objective of the lab session is to evaluate the performance of the embeddings learned in TP1 as well as a Transformer model (BERT) on the named entity task (NER) in the medical domain.
A NER model for each embeddings type will be learned and evaluated.&lt;/p&gt;
&lt;h1 id=&#34;ressources-1&#34;&gt;Ressources&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;The &lt;a href=&#34;https://saharghannay.github.io/files/scripts.zip&#34; target=&#34;_blank&#34;&gt; scripts &lt;/a&gt; to be adapted for the sequence classification task, one to run the LSTM and CNN models, another to run the Transformer (BERT) models&lt;/li&gt;
&lt;li&gt;Corpus: &lt;a href=&#34;https://perso.limsi.fr/neveol/TP_ISD2020.zip&#34;&gt;https://perso.limsi.fr/neveol/TP_ISD2020.zip&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;requirements&#34;&gt;Requirements&lt;/h1&gt;
&lt;p&gt;Please install the dependencies to run the scripts&lt;/p&gt;
&lt;p&gt;It is recommended to use Google Colab to work on the TP.&lt;/p&gt;
&lt;h1 id=&#34;quick-overview-of-the-datasets&#34;&gt;Quick overview of the datasets :&lt;/h1&gt;
&lt;p&gt;We will use a small medical corpus (QUAERO_FrenchMed) with named entity annotations.&lt;/p&gt;
&lt;p&gt;The dataset is provided in conll-like format and contain five columns separated by white spaces. Each word (or token) corresponds to a line and sentences are separated by an empty line.&lt;/p&gt;
&lt;p&gt;The columns correspond to: the token index within the sentence, the token, two columns that will not be used in this lab and finally, the last column contains the named entity tag.&lt;/p&gt;
&lt;p&gt;Named entity tags are in the I-TYPE format, which indicates that the word is part of a TYPE entity. If two entities follow each other, the first word of the second entity will get a B-TYPE tag to indicate that it is a new entity. The O tag indicates that the word is not part of an entity.&lt;/p&gt;
&lt;h1 id=&#34;ner-models-training&#34;&gt;NER models training&lt;/h1&gt;
&lt;p&gt;In order to do the work, it is necessary to adapt the scripts for the NER task (token classification): mainly the data loading, the objective function (if needed)&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In addition to that, in the cnn_classification.py script, the embeddings are initialized randomly, it is necessary to make the modifications to initialize them with the embeddings learned in TP1.&lt;/li&gt;
&lt;li&gt;In the transformer_classification script, it is necessary to change &amp;lsquo;AutoModelForSequenceClassification&amp;rsquo; to &amp;lsquo;AutoModelForTokenClassification&amp;rsquo;&lt;/li&gt;
&lt;li&gt;The results will be evaluated in terms of recall, precision and F1&lt;/li&gt;
&lt;li&gt;It is recommended to use seqeval  &lt;a href=&#34;https://pypi.org/project/seqeval/&#34;&gt;https://pypi.org/project/seqeval/&lt;/a&gt; for the evaluation&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;questions-1&#34;&gt;Questions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Which model provides the best performance on each dataset?&lt;/li&gt;
&lt;li&gt;Do you see any differences between the embeddings learned on two different corpora of TP1?&lt;/li&gt;
&lt;li&gt;Compare these results with a Transformer model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;recommandations-1&#34;&gt;Recommandations&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Report and scripts  to be produced and sent befor  the session of February  6th:&lt;/li&gt;
&lt;li&gt;It will summarize  the work of the two lab sessions&lt;/li&gt;
&lt;li&gt;For the second lab session, the report summarizes the NER results of the different models and the answer to the questions.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Semantic enrichment towards efficient speech representations</title>
      <link>https://saharghannay.github.io/publication/conference-paper/arxiv23/</link>
      <pubDate>Mon, 03 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/arxiv23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Explicit Knowledge Integration for Knowledge-Aware Visual Question Answering about Named Entities</title>
      <link>https://saharghannay.github.io/publication/conference-paper/acm23/</link>
      <pubDate>Mon, 12 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/acm23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Specialized Semantic Enrichment of Speech Representations</title>
      <link>https://saharghannay.github.io/publication/conference-paper/icassp23/</link>
      <pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/icassp23/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Continual self-supervised domain adaptation for end-to-end speaker diarization</title>
      <link>https://saharghannay.github.io/publication/conference-paper/slt2023/</link>
      <pubDate>Mon, 09 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/slt2023/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Benchmarking Transformers-based models on French Spoken Language Understanding tasks</title>
      <link>https://saharghannay.github.io/publication/conference-paper/is2022/</link>
      <pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/is2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Impact Analysis of the Use of Speech and Language Models Pretrained by Self-Supersivion for Spoken Language Understanding</title>
      <link>https://saharghannay.github.io/publication/conference-paper/lrec22_2/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/lrec22_2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The Spoken Language Understanding {MEDIA} Benchmark Dataset in the Era of Deep Learning: data updates, training and evaluation tools</title>
      <link>https://saharghannay.github.io/publication/conference-paper/lrec22_1/</link>
      <pubDate>Mon, 20 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/lrec22_1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Étude comparative de modèles Transformers en compréhension de la parole en français</title>
      <link>https://saharghannay.github.io/publication/conference-paper/jep_2022_1/</link>
      <pubDate>Mon, 13 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/jep_2022_1/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Le benchmark MEDIA revisité : données, outils et évaluation dans un contexte d&#39;apprentissage profond</title>
      <link>https://saharghannay.github.io/publication/conference-paper/jep_2022_2/</link>
      <pubDate>Mon, 13 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/jep_2022_2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>OVERLAP-AWARE LOW-LATENCY ONLINE SPEAKER DIARIZATION BASED ON END-TO-END LOCAL SEGMENTATION</title>
      <link>https://saharghannay.github.io/publication/conference-paper/asru-juan-2021/</link>
      <pubDate>Fri, 17 Dec 2021 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/asru-juan-2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluating the carbon footprint of NLP methods: a survey and analysis of existing tools</title>
      <link>https://saharghannay.github.io/publication/conference-paper/sustainlp_2021/</link>
      <pubDate>Wed, 10 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/sustainlp_2021/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Where Are We in Semantic Concept Extraction for Spoken Language Understanding?</title>
      <link>https://saharghannay.github.io/publication/conference-paper/specom-2021-sg/</link>
      <pubDate>Wed, 22 Sep 2021 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/specom-2021-sg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Networks approaches focused on French Spoken Language Understanding: application to the MEDIA Evaluation Task</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2020-coling/</link>
      <pubDate>Tue, 08 Dec 2020 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2020-coling/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Analyzing BERT Cross-lingual Transfer Capabilities in Continual Sequence Labeling</title>
      <link>https://saharghannay.github.io/publication/conference-paper/wp2022/</link>
      <pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/wp2022/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Error Analysis Applied to End-to-End Spoken Language Understanding</title>
      <link>https://saharghannay.github.io/publication/conference-paper/caubriere-2020-sluanaly/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/caubriere-2020-sluanaly/</guid>
      <description></description>
    </item>
    
    <item>
      <title>What is best for spoken language understanding: small but task-dependant embeddings or huge but out-of-domain embeddings?</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2020-sluembed/</link>
      <pubDate>Mon, 04 May 2020 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2020-sluembed/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Comparison of Metric Learning Loss Functions for End-To-End Speaker Verification</title>
      <link>https://saharghannay.github.io/publication/conference-paper/juan-2020-spk/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/juan-2020-spk/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Cooking Knowledge Graph and Benchmark for Question Answering Evaluation in Lifelong Learning Scenarios</title>
      <link>https://saharghannay.github.io/publication/conference-paper/mathilde_nldb_2020/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/mathilde_nldb_2020/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A Metric Learning Approach to Misogyny Categorization</title>
      <link>https://saharghannay.github.io/publication/conference-paper/juan-2020-metric/</link>
      <pubDate>Tue, 31 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/juan-2020-metric/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A study of continuous space word and sentence representations applied to ASR error detection</title>
      <link>https://saharghannay.github.io/publication/journal-article/ghannay-2020-spcom/</link>
      <pubDate>Sat, 07 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/journal-article/ghannay-2020-spcom/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lifelong learning and task-oriented dialogue system: what does it mean?</title>
      <link>https://saharghannay.github.io/publication/conference-paper/veron-2019-ll/</link>
      <pubDate>Wed, 24 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/veron-2019-ll/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://saharghannay.github.io/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-academic&#34;&gt;Create slides in Markdown with Academic&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/&#34;&gt;Academic&lt;/a&gt; | &lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
   One 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   **Two** 
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
   Three 
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/img/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/img/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://spectrum.chat/academic&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://sourcethemes.com/academic/docs/managing-content/#create-slides&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>End-to-end named entity and semantic concept extraction from speech</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2018-endtoend/</link>
      <pubDate>Tue, 18 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2018-endtoend/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Semantic Representations</title>
      <link>https://saharghannay.github.io/teachings/semantic-representations/</link>
      <pubDate>Fri, 05 Oct 2018 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/teachings/semantic-representations/</guid>
      <description>&lt;p&gt;Master 2 course, Université Paris-Saclay, IUT Orsay, Computer Science Department&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2018-2020&lt;/li&gt;
&lt;li&gt;MSc level (M2)&lt;/li&gt;
&lt;li&gt;Introduction to distributed representations, recent approaches, evaluation approaches&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;practical-session&#34;&gt;Practical session&lt;/h2&gt;
</description>
    </item>
    
    <item>
      <title>TED-LIUM 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation</title>
      <link>https://saharghannay.github.io/publication/conference-paper/hernandez-2018-tedlium/</link>
      <pubDate>Tue, 18 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/hernandez-2018-tedlium/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Service side web programming</title>
      <link>https://saharghannay.github.io/teachings/service-side-web-programming/</link>
      <pubDate>Wed, 05 Sep 2018 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/teachings/service-side-web-programming/</guid>
      <description>&lt;p&gt;BSc level (L2) course, Université Paris-Saclay, IUT Orsay, Computer Science Department&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;2018-2020&lt;/li&gt;
&lt;li&gt;BSc level (L2)&lt;/li&gt;
&lt;li&gt;PHP, MySQL queries, Object Oriented Programming, cookies, sessions&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Task Specific Sentence Embeddings for ASR Error Detection</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2018-sentem/</link>
      <pubDate>Thu, 02 Aug 2018 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2018-sentem/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Représentations de phrases dans un espace continu spécifiques à la tâche de détection d&#39;erreurs</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2018-sentemf/</link>
      <pubDate>Mon, 04 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2018-sentemf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Simulation d&#39;erreurs de reconnaissance automatique dans un cadre de compréhension de la parole</title>
      <link>https://saharghannay.github.io/publication/conference-paper/simonnet-2018-sluerrf/</link>
      <pubDate>Mon, 04 Jun 2018 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/simonnet-2018-sluerrf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>End-to-end named entity extraction from speech</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2018-endtoend2/</link>
      <pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2018-endtoend2/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Simulating ASR errors for training SLU systems</title>
      <link>https://saharghannay.github.io/publication/conference-paper/simonnet-2017-sluerr/</link>
      <pubDate>Mon, 07 May 2018 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/simonnet-2017-sluerr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Enriching confusion networks for post-processing</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2017-enriching/</link>
      <pubDate>Mon, 23 Oct 2017 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2017-enriching/</guid>
      <description></description>
    </item>
    
    <item>
      <title>A study of continuous word representations applied to the automatic detection of speech recognition errors</title>
      <link>https://saharghannay.github.io/publication/thesis/ghannay-2017-phd/</link>
      <pubDate>Wed, 27 Sep 2017 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/thesis/ghannay-2017-phd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ASR Error Management for Improving Spoken Language Understanding</title>
      <link>https://saharghannay.github.io/publication/conference-paper/simonnet-2017-slu/</link>
      <pubDate>Sun, 20 Aug 2017 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/simonnet-2017-slu/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Using Hypothesis Selection Based Features for Confusion Network MT System Combination</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2014-trans/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2014-trans/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Acoustic word embeddings for ASR error detection</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2016-acoustic/</link>
      <pubDate>Thu, 08 Sep 2016 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2016-acoustic/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Recent improvements on error detection for automatic speech recognition</title>
      <link>https://saharghannay.github.io/publication/conference-paper/esteve-2016-imp/</link>
      <pubDate>Tue, 30 Aug 2016 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/esteve-2016-imp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluation of acoustic word embeddings</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2016-acouseval/</link>
      <pubDate>Tue, 16 Aug 2016 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2016-acouseval/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Utilisation des représentations continues des mots et des paramètres prosodiques pour la détection d&#39;erreurs dans les transcriptions automatiques de la parole</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2016-errprossf/</link>
      <pubDate>Mon, 04 Jul 2016 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2016-errprossf/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Word embedding evaluation and combination</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2016-weevaluation/</link>
      <pubDate>Mon, 23 May 2016 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2016-weevaluation/</guid>
      <description></description>
    </item>
    
    <item>
      <title>External Project</title>
      <link>https://saharghannay.github.io/project/external-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/project/external-project/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Internal Project</title>
      <link>https://saharghannay.github.io/project/internal-project/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/project/internal-project/</guid>
      <description>&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.&lt;/p&gt;
&lt;p&gt;Nullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.&lt;/p&gt;
&lt;p&gt;Cras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.&lt;/p&gt;
&lt;p&gt;Suspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.&lt;/p&gt;
&lt;p&gt;Aliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Combining continous word representation and prosodic features for ASR error prediction</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2015-errpross/</link>
      <pubDate>Tue, 24 Nov 2015 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2015-errpross/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Which ASR errors are hard to detect?</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2015-erranalyse/</link>
      <pubDate>Fri, 11 Sep 2015 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2015-erranalyse/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Word embeddings combination and neural networks for robustness in ASR error detection</title>
      <link>https://saharghannay.github.io/publication/conference-paper/ghannay-2015-wecomberr/</link>
      <pubDate>Mon, 31 Aug 2015 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/publication/conference-paper/ghannay-2015-wecomberr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Algorithmic and programming</title>
      <link>https://saharghannay.github.io/project/cour-tranduction/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://saharghannay.github.io/project/cour-tranduction/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;2010-2015 (55 hours)&lt;/li&gt;
&lt;li&gt;BSc level (L2)&lt;/li&gt;
&lt;li&gt;Data structures (linked list, hashtable, tree), pointers, recursivity&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
