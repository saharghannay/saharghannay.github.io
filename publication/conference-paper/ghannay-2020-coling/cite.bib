@inproceedings{ghannay-2020-Coling,
    title = "Neural Networks approaches focused on {F}rench Spoken Language Understanding: application to the {MEDIA} Evaluation Task",
    author = "Ghannay, Sahar  and Servan, Christophe  and  Rosset, Sophie",
    booktitle = "Proceedings of the 28th International Conference on Computational Linguistics",
    month = "december",
    year = "2020",
    address = "Barcelona, Spain (Online)",
    publisher = "International Committee on Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.coling-main.245",
    doi = "10.18653/v1/2020.coling-main.245",
    pages = "2722--2727",  
    abstract = "In this paper, we present a study on a French Spoken Language Understanding (SLU) task: the MEDIA task. Many works and studies have been proposed for many tasks, but most of them are focused on English language and tasks. The exploration of a richer language like French within the framework of a SLU task implies to recent approaches to handle this difficulty. Since the MEDIA task seems to be one of the most difficult, according several previous studies, we propose to explore Neural Networks approaches focusing of three aspects: firstly, the Neural Network inputs and more specifically the word embeddings; secondly, we compared French version of BERT against the best setup through different ways; Finally, the comparison against State-of-the-Art approaches. Results show that the word embeddings trained on a small corpus need to be updated during SLU model training. Furthermore, the French BERT fine-tuned approaches outperform the classical Neural Network Architectures and achieves state of the art results. However, the contextual embeddings extracted from one of the French BERT approaches achieve comparable results in comparison to word embedding, when integrated into the proposed neural architecture.", 
}